{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ba99a0",
   "metadata": {},
   "source": [
    "<center>\n",
    " <img src = \"JHU.png\"  width=\"200\" alt=\"Johns Hopkins University logo\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e989856",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5f76378d7af512fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Heart Failure Disease Classification Using Ensemble Machine Learning Techniques\n",
    "\n",
    "Estimated time needed: **60** minutes\n",
    "\n",
    "### Overview:\n",
    "\n",
    "In this hands-on lab, we will generate ensembles of classifiers using the Bagging method and compare their performance to regular classifiers. We will perform classification on heart failure disease using the provided **heart_dataset**. The steps involve creating basic classifiers, forming ensembles, and evaluating their performance using cross-validation. The classifiers in focus are GaussianNB, LinearSVC, MLPClassifier, DecisionTreeClassifier, and RandomForestClassifier.\n",
    "\n",
    "### Learning objectives:\n",
    "\n",
    "- Load and preprocess data, including feature scaling and one-hot encoding.\n",
    "- Train and evaluate multiple classifiers using 10-fold cross-validation.\n",
    "- Build and evaluate ensemble models using the Bagging method.\n",
    "- Implement custom ensemble functions for subsampling and majority voting.\n",
    "- Compare and visualize performance between regular classifiers and ensemble models.\n",
    "\n",
    "\n",
    "### Dataset Information:\n",
    "\n",
    "   **heart_dataset** contains various features related to heart health and patient outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6435b6c8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a51e38630d3e3b13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 1: Load and Preprocess the Dataset\n",
    "- Use the `heart_dataset.csv` dataset from the module content.\n",
    "- Load the dataset into your development environment.\n",
    "- Examine the features and note that they include a mixture of numerical and nominal (categorical) data.\n",
    "- **Hint:** Use appropriate pre-processing techniques such as converting nominal data to numerical data using `OneHotEncoder`.\n",
    "- Ensure your preprocessing pipeline is correct. It might be helpful to start by running a baseline classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf7d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the heart failure dataset and preprocess it.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7144e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "df = pd.read_csv('heart_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b3014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here!\n",
    "# Encode categorical features\n",
    "categorical_cols = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "\n",
    "# Split into features and target\n",
    "X = df_encoded.drop('HeartDisease', axis=1)\n",
    "\n",
    "# Scale the features\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04678b97",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-14902b960ad495ca",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "<summary>Click here to view/hide solution.</summary>\n",
    "\n",
    "```   \n",
    "# Encode categorical features\n",
    "categorical_cols = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "# Split into features and target\n",
    "X = df_encoded.drop('HeartDisease', axis=1)\n",
    "y = df_encoded['HeartDisease']\n",
    "```\n",
    "\n",
    "```\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "```\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f60bb0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e1d0003aef0a946",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 2: 10-Fold Cross-Validation of Basic Classifiers\n",
    "-  Perform 10-fold cross-validation for the following classifiers using default parameters:\n",
    "  - **GaussianNB**\n",
    "  - **Linear SVC** (`SVC(kernel='linear', probability=True)`)\n",
    "  - **MLPClassifier**\n",
    "  - **DecisionTreeClassifier**\n",
    "- **Hint:** Use `cross_val_score` to perform cross-validation.\n",
    "- **Bonus:** Report the performance of `RandomForestClassifier` (no need for CV since itâ€™s already an ensemble).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d131160a",
   "metadata": {},
   "source": [
    "> **Note**: Running 10-fold cross-validation for each classifier, especially with models like RandomForestClassifier, may take a considerable amount of time. Please be patient while the computation completes, as it involves fitting and evaluating multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9026460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Perform 10-fold cross-validation on the specified classifiers and report their performance.\n",
    "# Initialize classifiers\n",
    "# Write your code here! \n",
    "\n",
    "classifiers = {\n",
    "\n",
    "    \n",
    "}\n",
    "# 10-fold CV for each classifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4700dd55",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-55c5c5ba63da5449",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "<summary>Click here to view/hide solution.</summary>\n",
    "\n",
    "```\n",
    "# Task: Perform 10-fold cross-validation on the specified classifiers and report their performance.\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'Linear SVC': SVC(kernel='linear', probability=True),\n",
    "    'MLPClassifier': MLPClassifier(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "    'RandomForestClassifier': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# 10-fold CV for each classifier\n",
    "cv_results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    if name != 'RandomForestClassifier':\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=10)\n",
    "        cv_results[name] = scores.mean()\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "        cv_results[name] = clf.score(X_test, y_test)\n",
    "\n",
    "print(cv_results)\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf9a14",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef2a4d8bad28fb2a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 3: Create Weak Classifiers and Ensembles\n",
    "-  Create an ensemble of 100 classifiers for each of the basic classifiers from Step 2.\n",
    "- To generate weak classifiers within your ensembles:\n",
    "  - For **MLPClassifier**, set hidden sizes to `(3, 3)`, max iterations to `30`, and tolerance to `1e-1`.\n",
    "  - For **DecisionTreeClassifier**, set max depth to `5` and max features to `5`.\n",
    "- **Task:** Report the performance of the first classifier in each ensemble.\n",
    "  - **Hint:** Just run the first classifier in each ensemble on your test data and compare results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724c7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import random\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d7113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here! \n",
    "# Generate an ensemble of 100 classifiers for the specified models with underpowered hyperparameters.\n",
    "\n",
    "def create_ensemble(base_classifier, n_estimators=100, **params):\n",
    "\n",
    "\n",
    "# Create weak ensembles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b77a36",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2e0c8dc5d143fa2e",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "<summary>Click here to view/hide solution.</summary>\n",
    "\n",
    "```\n",
    "# Generate an ensemble of 100 classifiers for the specified models with underpowered hyperparameters.\n",
    "    \n",
    "def create_ensemble(base_classifier, n_estimators=100, **params):\n",
    "    ensemble = [base_classifier(**params) for _ in range(n_estimators)]\n",
    "    return ensemble\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "# Create weak ensembles\n",
    "    \n",
    "mlp_ensemble = create_ensemble(MLPClassifier, hidden_layer_sizes=(3,3), max_iter=30, tol=1e-1)\n",
    "dt_ensemble = create_ensemble(DecisionTreeClassifier, max_depth=5, max_features=5)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc1b134",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-950b4bc3feaf4bd1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 4: Implement Ensemble Training with Bagging\n",
    "  - Write a function `ensemble_fit()` to train your ensemble using the Bagging method.\n",
    "  - **Hint:** Use `random.sample` to create subsets of your training data for each classifier in the ensemble.\n",
    "  - Ensure that each classifier only sees a different subset of the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42b0965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here! \n",
    "# Write a function to train the ensemble on subsets of the data.\n",
    "\n",
    "def ensemble_fit(ensemble, X_train, y_train, subsample_ratio=0.2):\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b4825",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here to view/hide solution.</summary>\n",
    "    \n",
    "```\n",
    "# Write a function to train the ensemble on subsets of the data.\n",
    "def ensemble_fit(ensemble, X_train, y_train, subsample_ratio=0.2):\n",
    "    n_samples = int(len(X_train) * subsample_ratio)\n",
    "    for clf in ensemble:\n",
    "        X_sub, y_sub = zip(*random.sample(list(zip(X_train, y_train)), n_samples))\n",
    "        clf.fit(X_sub, y_sub)\n",
    "\n",
    "ensemble_fit(mlp_ensemble, X_train, y_train, subsample_ratio=0.2)\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4164cc9",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-852dc1ad75c0fb98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 5: Implement Ensemble Prediction with Voting\n",
    "  -  Write a function `ensemble_predict()` to make predictions using your trained ensemble.\n",
    "  - **Hint:** Aggregate the predictions from all classifiers in the ensemble using a majority voting scheme.\n",
    "  - **Note:** Use `np.argmax()` to determine the final prediction from the votes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ea2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c211be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here! \n",
    "# Task: Implement the ensemble_predict() function to make predictions using the trained ensemble.\n",
    "def ensemble_predict(ensemble, X_test):\n",
    "\n",
    "\n",
    "# Example of predicting with one ensemble\n",
    "y_pred = ensemble_predict(mlp_ensemble, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5398c79a",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-812e7d0c877c44ec",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "<summary>Click here to view/hide solution.</summary>\n",
    "    \n",
    "```\n",
    "# Task: Implement the ensemble_predict() function to make predictions using the trained ensemble.\n",
    "\n",
    "def ensemble_predict(ensemble, X_test):\n",
    "    predictions = np.array([clf.predict(X_test) for clf in ensemble])\n",
    "    # Majority vote for the final prediction\n",
    "    final_prediction = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n",
    "    return final_prediction\n",
    "\n",
    "# Example of predicting with one ensemble\n",
    "y_pred = ensemble_predict(mlp_ensemble, X_test)\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8957f2b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-75726fdb442cfb2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 6: Evaluate Ensemble Performance with Different Subsample Ratios\n",
    "- Perform 10-fold CV for the ensembles with a subsample ratio of `0.2`.\n",
    "- Compare these results to a regular decision tree trained on the same subsample ratio.\n",
    "- Repeat the process with a subsample ratio of `0.05`.\n",
    "- **Hint:** You can use the same function from Step 4 to train on different subsample ratios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c7b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here! \n",
    "# Perform 10-fold CV for ensembles with different subsample ratios and compare them to regular classifiers.\n",
    "def evaluate_ensemble(ensemble, X_train, y_train, X_test, y_test, subsample_ratio):\n",
    "\n",
    "\n",
    "# Perform CV with different subsample ratios\n",
    "subsample_ratios = [0.2, 0.05]\n",
    "\n",
    "\n",
    "# Compare to regular classifiers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84ac87b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ffef56eb562b7166",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "<summary>Click here to view/hide solution.</summary>\n",
    "    \n",
    "```\n",
    "# Perform 10-fold CV for ensembles with different subsample ratios and compare them to regular classifiers.\n",
    "\n",
    "def evaluate_ensemble(ensemble, X_train, y_train, X_test, y_test, subsample_ratio):\n",
    "    ensemble_fit(ensemble, X_train, y_train, subsample_ratio=subsample_ratio)\n",
    "    y_pred = ensemble_predict(ensemble, X_test)\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    return accuracy\n",
    "\n",
    "# Perform CV with different subsample ratios\n",
    "subsample_ratios = [0.2, 0.05]\n",
    "ensemble_accuracies = {}\n",
    "for ratio in subsample_ratios:\n",
    "    ensemble_accuracies[f'MLP_{ratio}'] = evaluate_ensemble(mlp_ensemble, X_train, y_train, X_test, y_test, ratio)\n",
    "    ensemble_accuracies[f'DT_{ratio}'] = evaluate_ensemble(dt_ensemble, X_train, y_train, X_test, y_test, ratio)\n",
    "\n",
    "# Compare to regular classifiers\n",
    "regular_dt = DecisionTreeClassifier()\n",
    "regular_dt.fit(X_train, y_train)\n",
    "regular_accuracy = regular_dt.score(X_test, y_test)\n",
    "ensemble_accuracies['Regular_DT'] = regular_accuracy\n",
    "\n",
    "ensemble_accuracies\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3782fe",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-90cb45f4a556fed4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 7: Evaluate Performance Across Multiple Subsample Ratios\n",
    "- Report the 10-fold CV performances of the ensembles for subsample ratios of:\n",
    "  - `0.005`, `0.01`, `0.03`, `0.05`, `0.1`, `0.2`\n",
    "- Train regular versions of the classifiers from Step 2 on these subsample ratios and report their performance.\n",
    "  - **Hint:** You can use a list containing one element (the regular classifier) to pass through the same ensemble CV function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a98dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here! \n",
    "# Evaluate performance of ensembles at different subsample ratios and compare to regular classifiers.\n",
    "subsample_ratios = [0.005, 0.01, 0.03, 0.05, 0.1, 0.2]\n",
    "\n",
    "\n",
    "# Evaluate regular classifiers\n",
    "regular_results = {}\n",
    "for ratio in subsample_ratios:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e2be3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-adca92bfda0621d6",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "<summary>Click here to view/hide solution.</summary>\n",
    "    \n",
    "```\n",
    "# Evaluate performance of ensembles at different subsample ratios and compare to regular classifiers.\n",
    "\n",
    "subsample_ratios = [0.005, 0.01, 0.03, 0.05, 0.1, 0.2]\n",
    "ensemble_cv_results = {}\n",
    "\n",
    "for ratio in subsample_ratios:\n",
    "    ensemble_cv_results[f'MLP_{ratio}'] = evaluate_ensemble(mlp_ensemble, X_train, y_train, X_test, y_test, ratio)\n",
    "    ensemble_cv_results[f'DT_{ratio}'] = evaluate_ensemble(dt_ensemble, X_train, y_train, X_test, y_test, ratio)\n",
    "\n",
    "# Evaluate regular classifiers\n",
    "regular_results = {}\n",
    "for ratio in subsample_ratios:\n",
    "    regular_dt = DecisionTreeClassifier(max_depth=5, max_features=5)\n",
    "    regular_dt.fit(X_train, y_train)\n",
    "    regular_results[f'Regular_DT_{ratio}'] = regular_dt.score(X_test, y_test)\n",
    "\n",
    "ensemble_cv_results.update(regular_results)\n",
    "ensemble_cv_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3d50c8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-02bec7fd7b51ebf6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 8: Plot Performance Comparisons\n",
    "- For each classifier, plot the performance of the ensemble at different subsample ratios.\n",
    "- On the same plot, include the performance of the regular classifier at the same subsample ratios.\n",
    "- **Hint:** Use two different colors to distinguish between ensemble and regular classifier performances.\n",
    "- **Deliverable:** You should have four plots, one for each classifier type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3459a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f812e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here! \n",
    "# Plot the performance of the ensembles vs regular classifiers at different subsample ratios.\n",
    "def plot_performance(ensemble_cv_results, title):\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ffd8f1",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-720831d9408a0cf5",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "<summary>Click here to view/hide solution.</summary>\n",
    "    \n",
    "```\n",
    "# Plot the performance of the ensembles vs regular classifiers at different subsample ratios.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_performance(ensemble_cv_results, title):\n",
    "    subsample_ratios = [0.005, 0.01, 0.03, 0.05, 0.1, 0.2]\n",
    "    ensemble_acc = [ensemble_cv_results[f'MLP_{r}'] for r in subsample_ratios]\n",
    "    regular_acc = [ensemble_cv_results[f'Regular_DT_{r}'] for r in subsample_ratios]\n",
    "    \n",
    "    plt.plot(subsample_ratios, ensemble_acc, label='Ensemble', color='blue')\n",
    "    plt.plot(subsample_ratios, regular_acc, label='Regular', color='orange')\n",
    "    plt.xlabel('Subsample Ratio')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_performance(ensemble_cv_results, \"MLP vs Decision Tree Performance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8015459a",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "In this lab, you practiced building multiple classifiers and ensembles using the Bagging method, evaluating their performance using cross-validation. You learned how to create and evaluate weak classifiers, generate ensembles, subsample data, and compare the performance of ensemble models to regular classifiers.\n",
    "\n",
    "This process demonstrated how Bagging can improve model accuracy by reducing overfitting and improving stability through ensemble voting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
