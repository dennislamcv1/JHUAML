{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7611384c",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"JHU.png\" width=\"200\" alt=\"Johns Hopkins University logo\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff9e47",
   "metadata": {},
   "source": [
    "## Hands-on Lab: Clustering Algorithms\n",
    "\n",
    "Estimated time needed: **60** minutes\n",
    "\n",
    "### Overview:\n",
    "\n",
    "The primary objective of this assignment is to refine the data so that we can classify species together and use their features to classify new observations.\n",
    "\n",
    "In this lab, we will:\n",
    "\n",
    "- **Classify and Compare:** Discuss and compare clustering algorithms such as K-Means and DBSCAN and use it to cluster the data together and find anomalies, outliers or errors.\n",
    "\n",
    "- **Implement and Visualize a decision tree:** Implement a decision tree model to classify the species and then visualize the model decision tree. \n",
    "\n",
    "This lab is designed to deepen your understanding of machine learning concepts through practical application and comparison of different models and techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435a5c2",
   "metadata": {},
   "source": [
    "### Learning Objectives:\n",
    "\n",
    "In this lab, we aim to achieve the following objectives:\n",
    "\n",
    "- **Explore and Compare Clustering algorithms:** Provide a high-level overview of various machine learning clustering algorithms including K-Means clustering and DBSCAN (Density-Based Spatial Clustering of Applications with Noise). We will use these algorithms to cluster the data together and find anomalies, outliers or errors.\n",
    "\n",
    "\n",
    "- **Implement and visualize a decision tree:** Fit the data to a decision tree model based on the data given in clustering_synthetic_dataset.csv dataset. This will help classify the species and then visualize it.\n",
    "\n",
    "These objectives are designed to enhance your understanding of key machine learning concepts and their practical application in data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caa9a03",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "Let us first explore and compare two essential machine learning clustering algorithms: \n",
    "- K-Means\n",
    "- DBSCAN\n",
    "\n",
    "The focus will be on providing a high-level understanding of these algorithms, highlighting their strengths, weaknesses, and typical use cases.\n",
    "\n",
    "\n",
    "As we progress, We will guide you through the following aspects for each algorithm:\n",
    "\n",
    "- How they work: We'll give a brief description of how each of these algorithms work.\n",
    "- Advantages and disadvantages: We’ll evaluate the effectiveness of each algorithm in terms of when its advantageous to use.\n",
    "- When to use: We'll discuss when to use each of the algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd50aee",
   "metadata": {},
   "source": [
    "**K-Means:**\n",
    "- **How it works**: K-Means is a widely used clustering algorithm that partitions data points into K clusters based on their similarity.The algorithm works by iteratively updating the cluster centroids until convergence is achieved.\n",
    "\n",
    "- **Advantages**: \n",
    "                Easy to implement and interpret\n",
    "                Fast and efficient for large datasets\n",
    "- **When to Use**: K-Means clustering algorithm works well with spherical clusters.\n",
    "\n",
    "**DBSCAN(Density-Based Spatial Clustering of Applications with Noise):**\n",
    "\n",
    "- **How it works**: The algorithm starts by randomly selecting an unvisited point and checking if it has enough neighbors within a specified radius. If the point has enough nearby neighbors, it is marked as part of a cluster. The algorithm then recursively checks if the neighbors also have enough neighbors within the radius, until all points in the cluster have been visited. Points that are not part of any cluster are marked as noise.\n",
    "\n",
    "- **Advantages**: One of the advantages of DBSCAN is that it can find clusters of arbitrary shapes and sizes, unlike K-Means which assumes spherical clusters.DBSCAN is also robust to noise and outliers since they are not assigned to any cluster. However, DBSCAN can be sensitive to the choice of distance metric and parameters such as the radius and minim number of points required to form a cluster.\n",
    "\n",
    "- **When to Use**: DBSCAN works wellwith all kinds of clusters of arbitrary shapes and sizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8653124d",
   "metadata": {},
   "source": [
    "### Data Description:\n",
    "\n",
    "The provided clustering_synthetic_dataset.csv contains two features for observations that can be grouped into different species. The task is to determine how many species are present and classify the data using clustering techniques, while handling anomalies and outliers to refine the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16f4bcd",
   "metadata": {},
   "source": [
    "### Problem statement\n",
    "\n",
    "**Implement clustering algorithms such as K-Means and DBSCAN to find anomalies, errors and outliers in the dataset clustering_synthetic_dataset.csv file. Visualize the clusters, remove these outliers and fit a decision tree model to the data and visualize it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e47f2a",
   "metadata": {},
   "source": [
    "Let’s start by loading the dataset and implementing these steps in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7700dda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.494261</td>\n",
       "      <td>1.451067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.428081</td>\n",
       "      <td>-0.837064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.338559</td>\n",
       "      <td>1.038759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119001</td>\n",
       "      <td>-1.053976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.122425</td>\n",
       "      <td>1.774937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1        f2\n",
       "0  0.494261  1.451067\n",
       "1 -1.428081 -0.837064\n",
       "2  0.338559  1.038759\n",
       "3  0.119001 -1.053976\n",
       "4  1.122425  1.774937"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'clustering_synthetic_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e8ed5c",
   "metadata": {},
   "source": [
    "The dataset contains the 750 rows of data with the X coordinate and the Y coordinate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9709035b",
   "metadata": {},
   "source": [
    "**Task 1: Plot the data with a scatterplot. How many species must be there in the dataset?(For the rest of this assignment, use that number as the number-of-clusters parameter in\n",
    "methods such as KMeans)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce3a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d372b313",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "# There must be 3 species in the dataset.\n",
    "plt.scatter(df['f1'], df['f2'], c='blue', label='Normal points')\n",
    "\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8c37ce",
   "metadata": {},
   "source": [
    "**Explanation**: The scatterplot helps visually estimate the number of species (clusters) by grouping observations based on their feature values. By observing the scatterplot, you can roughly estimate the number of clusters present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db983a5f",
   "metadata": {},
   "source": [
    "**Task 2: Find the rough feature ranges to classify these species correctly. It might be a good\n",
    "idea to do this step visually from some data plots.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27abf0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 f1            f2\n",
      "count  7.500000e+02  7.500000e+02\n",
      "mean   5.217752e-15 -7.853274e-14\n",
      "std    1.000667e+00  1.000667e+00\n",
      "min   -2.274474e+00 -1.823801e+00\n",
      "25%   -1.091894e+00 -7.775487e-01\n",
      "50%    3.886712e-01 -4.095144e-01\n",
      "75%    7.787849e-01  1.052538e+00\n",
      "max    1.870438e+00  2.245794e+00\n"
     ]
    }
   ],
   "source": [
    "# Display basic statistics\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5f5f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms to understand feature ranges\n",
    "# Write your code here\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f2e376",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    "\n",
    "```    \n",
    "# Plotting histograms to understand feature ranges\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df.iloc[:, 0], bins=30, color='skyblue')  # Changed data to df\n",
    "plt.title('Distribution of Feature 1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df.iloc[:, 1], bins=30, color='salmon')  # Changed data to df\n",
    "plt.title('Distribution of Feature 2')\n",
    "plt.xlabel('Feature 2')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae008002",
   "metadata": {},
   "source": [
    "**Explanation**: By examining the summary statistics and the code is used to visualize the distribution of Feature 1 and Feature 2 through histograms, which help in identifying rough feature ranges for different species. The distribution of data points across these features can give insights into how the species are grouped.\n",
    "\n",
    "However, **note** that this alone might not directly link the ranges to specific species. To refine your understanding of the species, you may need to complement this analysis with clustering techniques (e.g., KMeans or DBSCAN) to further identify which range belongs to which species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3cb59b",
   "metadata": {},
   "source": [
    "> **Note**: In the next set of problems, we will clean the points that are around the boundaries of the\n",
    "cluster. (These points might be due to errors, anomalies, or simply be outliers.) This step is\n",
    "done to refine feature boundaries so that a scientist can classify the species manually,\n",
    "reliably, and with a high-level generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c18f989",
   "metadata": {},
   "source": [
    "**Task 3: Use K-means clustering to find anomalies. (Hint: find cluster data points that are far\n",
    "from the centroids.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9639859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727729e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "#Apply K-Means\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(df)\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Compute distances from centroids\n",
    "\n",
    "\n",
    "# Define a threshold for anomaly detection\n",
    "\n",
    "\n",
    "# Identify anomalies\n",
    "\n",
    "\n",
    "# Plot results(optional)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51236bf5",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "\n",
    "#Apply K-Means\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(df)\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "# Compute distances from centroids\n",
    "distances = np.linalg.norm(df - centroids[labels], axis=1)\n",
    "\n",
    "# Define a threshold for anomaly detection\n",
    "threshold = np.percentile(distances, 95)  # 95th percentile distance as threshold\n",
    "\n",
    "# Identify anomalies\n",
    "anomalies = distances > threshold\n",
    "\n",
    "# Plot results\n",
    "data_scaled = df.to_numpy()\n",
    "plt.scatter(df['f1'],df['f2'], c='blue', label='Normal points')\n",
    "plt.scatter(data_scaled[anomalies, 0], data_scaled[anomalies, 1], c='red', label='Anomalies')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=100, label='Centroids')\n",
    "plt.legend()\n",
    "plt.title('K-means Clustering and Anomaly Detection')\n",
    "plt.show()\n",
    "\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c22e7",
   "metadata": {},
   "source": [
    "**Explanation**: After applying K-means clustering, we calculate the Euclidean distance from each data point to its cluster's centroid. Points with the largest distances are considered anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a21121",
   "metadata": {},
   "source": [
    "**Task 4: Use DBSCAN clustering to find anomalies. To be clear, look for anomalies with\n",
    "DBSCAN in the full dataset; this is an alternative to Q3.’s method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cea61b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a897557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "labels = dbscan.fit_predict(df)\n",
    "# Identify anomalies\n",
    "\n",
    "\n",
    "# Plot results(optional)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb21729",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "labels = dbscan.fit_predict(df)\n",
    "    \n",
    "# Identify anomalies\n",
    "anomalies = (labels == -1)\n",
    "    \n",
    "# Plot results\n",
    "data_scaled = df.to_numpy()\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(df['f1'],df['f2'], c='blue', label='Normal points')\n",
    "plt.scatter(data_scaled[anomalies, 0], data_scaled[anomalies, 1], c='red', label='Anomalies')\n",
    "plt.title('DBSCAN Clustering and Anomaly Detection')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c41930e",
   "metadata": {},
   "source": [
    "**Explanation**: DBSCAN (Density-Based Spatial Clustering) identifies anomalies by labeling points that do not belong to any dense region as -1. These are treated as anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d92f1dc",
   "metadata": {},
   "source": [
    "**Task 5: Fit the Decision Tree**\n",
    "\n",
    "Now, choose either the K-means results from Q3. or the DBSCAN results from Q4.,\n",
    "remove the points that the chosen method deemed anomalous, and train a decision tree\n",
    "from the remaining data to classify the species. (You do not need to justify the choice; they\n",
    "should both be reasonable options.) Visualize the model decision tree (but not just by\n",
    "plotting lines on a scatterplot of the data). Hint: the result should look like Module 6’s\n",
    "Jupyter Notebook’s cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a4fa097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column of labels\n",
    "df['label'] = dbscan.labels_\n",
    "# Filter out anomalies\n",
    "data_cleaned = df[df['label'] != -1]\n",
    "# Separate features and target variable\n",
    "X = data_cleaned.drop(columns=['label'])\n",
    "y = data_cleaned['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "# Write your code here\n",
    "\n",
    "# Create and train the Decision Tree model\n",
    "# Write your code here\n",
    "\n",
    "# Make predictions\n",
    "# Write your code here\n",
    "\n",
    "# Evaluate the model\n",
    "# Write your code here\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda4835",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "\n",
    "# Create a new column of labels\n",
    "df['label'] = dbscan.labels_\n",
    "# Filter out anomalies\n",
    "data_cleaned = df[df['label'] != -1]\n",
    "# Separate features and target variable\n",
    "X = data_cleaned.drop(columns=['label'])\n",
    "y = data_cleaned['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(dt_model, feature_names=X.columns, filled=True, rounded=True)\n",
    "plt.show()\n",
    "\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b8342c",
   "metadata": {},
   "source": [
    "**Explanation**: After removing the anomalies, we train a decision tree on the remaining data. The decision tree is visualized, showing how the model splits the data based on feature values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686675bb",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "In this assignment, we applied K-Means and DBSCAN clustering techniques to identify and remove anomalies in a synthetic dataset. After cleaning the data, we trained a decision tree to classify the species and demonstrated the improvement in accuracy by removing outliers. This exercise underscores the importance of handling anomalies for better model performance and generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
