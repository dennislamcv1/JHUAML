{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0434cd4",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"JHU.png\" width=\"200\" alt=\"Johns Hopkins University logo\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b498d1fb",
   "metadata": {},
   "source": [
    "# Hands-on Lab: Cyber Intrusion Detection Systems\n",
    "\n",
    "Estimated time needed: **60** minutes\n",
    "\n",
    "### Overview:\n",
    "\n",
    "In this lab, we will develop a machine learning model for detecting cybersecurity intrusions using seven datasets from the CICIDS 2017 collection. Each dataset represents a different type of cyber-attack captured over five days. We will focus on Dataset 1, ensuring it has multiple class values, and apply a Random Forest classifier due to its robustness and ability to handle feature importance. We will process the class feature into binary values, explore key network features, visualize the data, evaluate different classifiers, and adapt our approach for the remaining datasets. Finally, we will reflect on the challenges of developing a machine learning model in the field of cybersecurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c006b",
   "metadata": {},
   "source": [
    "### Learning Objectives:\n",
    "\n",
    "In this lab, we aim to achieve the following objectives:\n",
    "\n",
    "- Learn to preprocess and manipulate datasets effectively.\n",
    "- Explore and engineer relevant features for network traffic analysis.\n",
    "- Select and justify appropriate machine learning methodologies.\n",
    "- Evaluate machine learning models using cross-validation and performance metrics.\n",
    "- Utilize data visualization techniques to identify patterns in datasets.\n",
    "- Adapt code for application across multiple datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64d66bc",
   "metadata": {},
   "source": [
    "### Datasets Used:\n",
    "\n",
    "The datasets consist of network traffic data captured during various timeframes. Each dataset includes the following columns:\n",
    "\n",
    "- **Flow ID:** Unique identifier for each flow.\n",
    "- **Source IP/Port:** IP address and port of the source.\n",
    "- **Destination IP/Port:** IP address and port of the destination.\n",
    "- **Protocol:** The transport protocol used (e.g., TCP, UDP).\n",
    "- **Timestamp:** The time at which the flow was recorded.\n",
    "- **Flow Duration:** Total time duration of the flow.\n",
    "- **Total Fwd/Backward Packets:** Total number of packets sent in the forward and backward directions.\n",
    "- **Total Length of Fwd/Bwd Packets:** Total bytes sent in the forward and backward directions.\n",
    "- **Fwd/Bwd Packet Length Stats:** Various statistics about packet lengths (max, min, mean, standard deviation).\n",
    "- **Flow Bytes/s and Packets/s:** Throughput metrics for the flow.\n",
    "- **Flow IAT (Inter-Arrival Time):** Time intervals between packets, with various statistics.\n",
    "- **Flags and Counts:** Information about TCP flags and their counts.\n",
    "- **Packet Length Stats:** Additional statistics regarding packet lengths, including variance.\n",
    "- **Window Sizes and Bulk Metrics:** Metrics related to the forwarding and backward window sizes and data transfer rates.\n",
    "- **Active/Idle Times:** Statistics for active and idle periods during the flow.\n",
    "- **Label:** Class label indicating whether the flow is benign or malicious.\n",
    "\n",
    "All the seven datasets contain similar structures and types of information, allowing for consistent analysis across different timeframes and conditions. Out of the seven datasets, we will be working on four in this lab, and you will need to practice with the remaining three."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a64595b",
   "metadata": {},
   "source": [
    "### Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f0110",
   "metadata": {},
   "source": [
    "#### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207d1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87f7847",
   "metadata": {},
   "source": [
    "#### Problem 1: Pick one of the data files, call it Dataset 1, and examine its features. Make sure it has more than one class value for its label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba486e8",
   "metadata": {},
   "source": [
    "To begin, we have chosen \"Tuesday-WorkingHours.pcap_ISCX.csv\" as Dataset 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72993f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_1 = pd.read_csv('Tuesday-WorkingHours.pcap_ISCX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15881d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading and trailing whitespace from all column names in dataset_1\n",
    "# write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff08dee",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "dataset_1.columns = dataset_1.columns.str.strip()\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ab1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the features of the dataset and determine if there is a suitable label column with multiple classes.\n",
    "\n",
    "# print the column names for dataset_1\n",
    "# write your code here\n",
    "\n",
    "\n",
    "# Check for unique values in each column to identify potential label columns\n",
    "# write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb68606",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "# print the column names for dataset_1\n",
    "print(dataset_1.columns)\n",
    "\n",
    "# Check for unique values in each column to identify potential label columns\n",
    "for col in dataset_1.columns:\n",
    "    print(f\"{col}: {dataset_1[col].nunique()} unique values\")    \n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2735c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label column name and print its unique values from dataset_1\n",
    "# write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8936ca79",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "label_column = 'Label'  \n",
    "print(f\"Unique values in {label_column}: {dataset_1[label_column].unique()}\")\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a16e111",
   "metadata": {},
   "source": [
    "**For Dataset 1, Random Forest is an excellent choice for classification due to its ability to handle high-dimensional data without scaling and its robustness against overfitting through the use of multiple decision trees. It effectively manages class imbalances with techniques like class weighting and provides feature importance rankings, offering insights into key attributes. Additionally, its non-parametric nature allows it to accommodate the non-linear patterns often seen in network traffic.**    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af59d01",
   "metadata": {},
   "source": [
    "#### Problem 2: Process the class feature/category as binary classes for supervised learning, assign BENIGN to value 0 and the rest to value 1. Check its balance for the Dataset 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bbce52",
   "metadata": {},
   "source": [
    "To process the class feature of Dataset 1 as binary classes (assigning BENIGN to 0 and all other values to 1), follow these steps:\n",
    "\n",
    "- Identify the target column: The class column is labeled as \"Label.\"\n",
    "- Assign binary values: Convert BENIGN to 0 and all other class values to 1.\n",
    "- Check balance: After processing, count the number of instances in each class to assess for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e9a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign binary values in the 'Label' column: 'BENIGN' as 0 and all other values as 1\n",
    "dataset_1['Label_binary'] = dataset_1['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)\n",
    "\n",
    "# Check the balance of the binary classes\n",
    "# write your code here\n",
    "\n",
    "\n",
    "# Display the class balance\n",
    "# write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae9c4cd",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "# Check the balance of the binary classes\n",
    "class_counts = dataset_1['Label_binary'].value_counts()\n",
    "\n",
    "# Display the class balance\n",
    "print(\"Class balance:\")\n",
    "print(class_counts)\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69599adb",
   "metadata": {},
   "source": [
    "#### Problem 3: Explore Dataset 1 features with respect to the class. \n",
    "**(Hint: features Source Port and Destination Port are very useful; research and find out important networking port numbers and one-hot-encode them. Unimportant port numbers or source port numbers can be assigned to a feature called 'other ports'.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of important ports (common port numbers for HTTP, HTTPS, FTP, SSH, etc.)\n",
    "important_ports = [80, 443, 21, 22, 53, 25, 110, 143]  # Modify this list based on your research\n",
    "\n",
    "# Function to label ports: important ports retain their number, others are labeled as 'Other Port'\n",
    "def label_ports(port, important_ports):\n",
    "    if port in important_ports:\n",
    "        return port\n",
    "    else:\n",
    "        return 'Other Port'\n",
    "\n",
    "# Apply the labeling function to both 'Source Port' and 'Destination Port'\n",
    "# write your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# One-Hot Encoding the 'Source Port Label' and 'Destination Port Label' columns\n",
    "# write your code here\n",
    "\n",
    "\n",
    "\n",
    "# Drop the original label columns for source and destination ports\n",
    "# write your code here\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate the one-hot encoded columns back to the dataset\n",
    "# write your code here\n",
    "\n",
    "\n",
    "\n",
    "# Display the first few rows of the updated dataset to check the result\n",
    "# write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dda2a11",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "# Apply the labeling function to both 'Source Port' and 'Destination Port'\n",
    "dataset_1['Source Port Label'] = dataset_1['Source Port'].apply(lambda x: label_ports(x, important_ports))\n",
    "dataset_1['Destination Port Label'] = dataset_1['Destination Port'].apply(lambda x: label_ports(x, important_ports))\n",
    "\n",
    "# One-Hot Encoding the 'Source Port Label' and 'Destination Port Label' columns\n",
    "source_port_encoded = pd.get_dummies(dataset_1['Source Port Label'], prefix='Source_Port')\n",
    "destination_port_encoded = pd.get_dummies(dataset_1['Destination Port Label'], prefix='Destination_Port')\n",
    "\n",
    "# Drop the original label columns for source and destination ports\n",
    "dataset_1.drop(['Source Port Label', 'Destination Port Label'], axis=1, inplace=True)\n",
    "\n",
    "# Concatenate the one-hot encoded columns back to the dataset\n",
    "dataset_1 = pd.concat([dataset_1, source_port_encoded, destination_port_encoded], axis=1)\n",
    "\n",
    "# Display the first few rows of the updated dataset to check the result\n",
    "print(dataset_1.head())\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f22270",
   "metadata": {},
   "source": [
    "#### Problem 4: Display some histograms and anything you deem fit to pick independent Dataset 1 features.\n",
    "**(Hint: source/destination bytes, packets, ports and the duration features.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989c9408",
   "metadata": {},
   "source": [
    "**Problem 4.1: Plot a boxplot for Total Backward Packets, excluding outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df17ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new figure with specified width and height\n",
    "plt.figure(figsize=(8, 4))  \n",
    "\n",
    "# Create a boxplot for 'Total Backward Packets', hiding outliers\n",
    "# write your code here\n",
    "\n",
    "\n",
    "# Set the title and label for the x-axis\n",
    "# write your code here\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "# write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828690d0",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "# Create a boxplot for 'Total Backward Packets', hiding outliers\n",
    "sns.boxplot(x=dataset_1['Total Backward Packets'], showfliers=False)\n",
    "\n",
    "# Set the title and label for the x-axis\n",
    "plt.title('Box Plot of Total Backward Packets (Without Outliers)')\n",
    "plt.xlabel('Total Backward Packets')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dba797",
   "metadata": {},
   "source": [
    "#### Problem 4.2: Plot a histogram for Flow Duration with a kernel density estimate (KDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new figure with specified width and height for the histogram\n",
    "plt.figure(figsize=(7, 5))  \n",
    "\n",
    "# Create a histogram of Flow Duration, including a KDE to visualize the distribution shape\n",
    "# write your code here\n",
    "\n",
    "\n",
    "# Set the title of the plot\n",
    "# write your code here\n",
    "\n",
    "\n",
    "# Label the x-axis and y-axis\n",
    "# write your code here\n",
    "  \n",
    "\n",
    "# Display the plot\n",
    "# write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9271eaf",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "# Create a histogram of Flow Duration, including a KDE to visualize the distribution shape\n",
    "sns.histplot(dataset_1['Flow Duration'], bins=20, kde=True)\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title('Flow Duration Distribution') \n",
    "\n",
    "# Label the x-axis and y-axis\n",
    "plt.xlabel('Flow Duration')  \n",
    "plt.ylabel('Frequency')  \n",
    "\n",
    "# Display the plot\n",
    "plt.show()  \n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca51fc",
   "metadata": {},
   "source": [
    "#### Problem 4.3: Plot Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4fa7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for the heatmap\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# Calculate the correlation matrix for selected features\n",
    "# write your code here\n",
    "\n",
    "\n",
    "\n",
    "# Plot the heatmap with annotations, using a color map for visualization\n",
    "# write your code here\n",
    "\n",
    "\n",
    "# Set the title of the heatmap\n",
    "# write your code here\n",
    "\n",
    "\n",
    "# Display the heatmap\n",
    "# write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c70e0fe",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "# Calculate the correlation matrix for selected features\n",
    "correlation_matrix = dataset_1[['Total Length of Fwd Packets', \n",
    "                                  'Total Length of Bwd Packets', \n",
    "                                  'Total Fwd Packets', \n",
    "                                  'Total Backward Packets', \n",
    "                                  'Flow Duration']].corr()\n",
    "\n",
    "# Plot the heatmap with annotations, using a color map for visualization\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1)\n",
    "\n",
    "# Set the title of the heatmap\n",
    "plt.title('Correlation Heatmap')\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306963c2",
   "metadata": {},
   "source": [
    "#### Problem 5: Attempt a few classifier models and report their 10-fold CV performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63449687",
   "metadata": {},
   "source": [
    "**Problem 5.1: Prepare the Data for Classifier Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06222305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features for model training\n",
    "features = ['Total Length of Fwd Packets', 'Total Length of Bwd Packets', \n",
    "            'Total Fwd Packets', 'Total Backward Packets', 'Flow Duration']\n",
    "\n",
    "# Define the feature set and target variable\n",
    "# write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa481b",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "X = dataset_1[features]  \n",
    "y = dataset_1['Label']\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b82e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels if they are categorical\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode labels\n",
    "# write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ea6d9a",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "y_encoded = label_encoder.fit_transform(y)  \n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaad433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values and standardize features using a pipeline\n",
    "pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),  \n",
    "    StandardScaler()  \n",
    ")\n",
    "\n",
    "# Apply the transformations to the feature set X.\n",
    "# write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657687ca",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "X_processed = pipeline.fit_transform(X) \n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c39bef3",
   "metadata": {},
   "source": [
    ">  **Note: Please be patient as the execution may take some time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6743a7",
   "metadata": {},
   "source": [
    "**Problem 5.2: Evaluate Classifier Models and Report 10-Fold Cross-Validation Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd661d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to hold results\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5616975d",
   "metadata": {},
   "source": [
    "**1. Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21383a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression model and perform 10-fold cross-validation.\n",
    "# write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b56730",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "cv_scores_log_reg = cross_val_score(log_reg, X_processed, y_encoded, cv=10, scoring='accuracy', n_jobs=-1)\n",
    "results['Logistic Regression'] = cv_scores_log_reg.mean()\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075299e2",
   "metadata": {},
   "source": [
    "**2. Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde623a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Decision Tree model and perform 10-fold cross-validation.\n",
    "# write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91c8de",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "cv_scores_decision_tree = cross_val_score(decision_tree, X_processed, y_encoded, cv=10, scoring='accuracy', n_jobs=-1)\n",
    "results['Decision Tree'] = cv_scores_decision_tree.mean()\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299dc9a7",
   "metadata": {},
   "source": [
    "**3. Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa438a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest model and perform 10-fold cross-validation.\n",
    "# write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76560cd4",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "random_forest = RandomForestClassifier()\n",
    "cv_scores_random_forest = cross_val_score(random_forest, X_processed, y_encoded, cv=10, scoring='accuracy', n_jobs=-1)\n",
    "results['Random Forest'] = cv_scores_random_forest.mean()\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad5ee8",
   "metadata": {},
   "source": [
    "#### Problem 5.3: Compile Results into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results dictionary to a DataFrame for easier visualization\n",
    "results_df = pd.DataFrame(list(results.items()), columns=['Classifier', '10-Fold CV Accuracy'])\n",
    "\n",
    "# Display the results DataFrame\n",
    "# write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c3260",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "print(results_df)\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f98390a",
   "metadata": {},
   "source": [
    "#### Problem 6: Adapt your code to analyze any three datasets from the remaining 6 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe14c41c",
   "metadata": {},
   "source": [
    "For this task, you need to utilize the following three datasets:\n",
    "\n",
    "- **dataset_2:** Wednesday-workingHours.pcap_ISCX.csv \n",
    "- **dataset_3:** Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv  \n",
    "- **dataset_4:** Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ae360d",
   "metadata": {},
   "source": [
    "**Data Preparation**\n",
    "\n",
    "To ensure consistency in the datasets, you need to clean the column names and load the data properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642054ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean column names by removing leading and trailing whitespace\n",
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.strip()  # Strip whitespace from column names\n",
    "    return df\n",
    "\n",
    "# Function to load the dataset, handling potential encoding issues\n",
    "# Attempt to load with UTF-8 encoding\n",
    "# Fallback to ISO-8859-1 encoding\n",
    "# write your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1017a327",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "def load_dataset(file):\n",
    "    try:\n",
    "        return pd.read_csv(file, encoding='utf-8')  \n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(file, encoding='ISO-8859-1')  \n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc4e78",
   "metadata": {},
   "source": [
    "#### Problem 7: Pick a classifier algorithm and report its evaluation for the 3 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29213d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_supervised_model(dataset):\n",
    "    # Clean the dataset column names\n",
    "    dataset = clean_column_names(dataset)\n",
    "    \n",
    "    # Define the required features for modeling\n",
    "    required_features = ['Total Length of Fwd Packets', 'Total Length of Bwd Packets',\n",
    "                         'Total Fwd Packets', 'Total Backward Packets', 'Flow Duration', 'Label']\n",
    "    \n",
    "    # Check for any missing features in the dataset\n",
    "    missing_features = [feature for feature in required_features if feature not in dataset.columns]\n",
    "    if missing_features:\n",
    "        return f\"Cannot evaluate: missing features {missing_features}\"\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X = dataset[required_features[:-1]]  # Features excluding the label\n",
    "    y_encoded = LabelEncoder().fit_transform(dataset['Label'])  # Encode the target labels\n",
    "\n",
    "    # Identify numeric and categorical features\n",
    "    numeric_features = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Create a preprocessing pipeline for numeric and categorical features\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', Pipeline([('imputer', SimpleImputer(strategy='mean')),  # Handle missing values with mean\n",
    "                          ('scaler', StandardScaler())]), numeric_features),  # Standardize numeric features\n",
    "        ('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing categorical values\n",
    "                          ('onehot', OneHotEncoder(handle_unknown='ignore'))]), categorical_features)  # One-hot encode categorical features\n",
    "    ])\n",
    "\n",
    "    \n",
    "    # Create a modeling pipeline that includes preprocessing and classifier\n",
    "    # Use Random Forest as the classifier\n",
    "    # write your code here\n",
    "    \n",
    "    \n",
    "    \n",
    "                             \n",
    "    # Evaluate the model using 10-fold cross-validation and return the mean accuracy\n",
    "    return cross_val_score(model, X, y_encoded, cv=10, scoring='accuracy', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36152f5",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                             ('classifier', RandomForestClassifier())]) \n",
    "\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95ff5e",
   "metadata": {},
   "source": [
    "**Evaluate Supervised Datasets and Compile Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc88575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised dataset paths for Datasets 2-4\n",
    "supervised_files = [\n",
    "    'Wednesday-workingHours.pcap_ISCX.csv',\n",
    "    'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
    "    'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',  \n",
    "]\n",
    "\n",
    "# Create new dataset names for supervised datasets\n",
    "dataset_names = [f'dataset_{i + 2}' for i in range(len(supervised_files))]  # Generates ['dataset_2', 'dataset_3', 'dataset_4']\n",
    "\n",
    "# Evaluate supervised datasets\n",
    "supervised_results = {\n",
    "    name: evaluate_supervised_model(load_dataset(file))\n",
    "    for name, file in zip(dataset_names, supervised_files)\n",
    "}\n",
    "\n",
    "# Create a DataFrame for supervised results\n",
    "# write your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a40a3e",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "supervised_results_df = pd.DataFrame(list(supervised_results.items()), columns=['Dataset', '10-Fold CV Accuracy'])\n",
    "print(\"Supervised Learning Results:\")\n",
    "print(supervised_results_df)\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ead74c",
   "metadata": {},
   "source": [
    "### Practice datasets:\n",
    "\n",
    "#### Next Steps: \n",
    "To enhance your skills in cybersecurity intrusion detection, we have provided three additional datasets for you to practice. You can apply the supervised learning algorithms and techniques you've learned, such as data preprocessing, feature engineering, and model evaluation, to these new datasets.\n",
    "\n",
    "**Datasets:**\n",
    "\n",
    "- **Friday-WorkingHours-Morning.pcap_ISCX.csv**\n",
    "- **Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv**\n",
    "- **Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv**\n",
    "\n",
    "Experiment with these datasets by implementing similar approaches and observing how different attack types and network characteristics affect model performance. This will help reinforce your understanding and ability to adapt your methods in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2772de6",
   "metadata": {},
   "source": [
    "> **Note**: To practice, create a new notebook by navigating to File > New Notebook and open the link in a new tab.\n",
    "The datasets are already loaded into the environment, so you can easily access them using the same names as mentioned above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c49ee0f",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "In this lab, we developed a machine learning model for cybersecurity intrusion detection using CICIDS 2017 datasets. We focused on preprocessing the data, exploring key features, and implementing multiple classifiers, including a Random Forest. We evaluated their performance through cross-validation. This exercise underscored the challenges of developing effective models in cybersecurity and highlighted the importance of systematic data handling and model evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
