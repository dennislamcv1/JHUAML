{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a6c35ce",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"JHU.png\" width=\"200\" alt=\"Johns Hopkins University logo\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c20ca",
   "metadata": {},
   "source": [
    "## Mining Patterns in Alice in Wonderland & Building a Neural Network on MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee117d",
   "metadata": {},
   "source": [
    "### Overview:\n",
    "\n",
    "Estimated time needed: **60** minutes\n",
    "\n",
    "This hands-on lab covers two topics:\n",
    "1. **A priori analysis** using the novel *Alice in Wonderland* by Lewis Carroll. You will mine frequent patterns and phrases in the novel, such as \"Mock Turtle\" or \"White Rabbit,\" using the Apriori algorithm and apply it using the **nltk** library. You will preprocess the text data and export it into a format readable by machine learning frameworks like Weka or use Python-based libraries like **mlxtend**.\n",
    "2. **Neural Network Implementation** using the MNIST dataset to recognize handwritten digits. You will implement and evaluate a neural network using the class **NeuralNetMLP** with two hidden layers and assess its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cec244",
   "metadata": {},
   "source": [
    "## Part 1: A Priori Analysis of Alice in Wonderland"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a82a4",
   "metadata": {},
   "source": [
    "### Dataset:\n",
    "The dataset for this task is the *Alice in Wonderland* novel available in the NLTK Gutenberg corpus. You will treat each sentence as a transaction and the words within it as items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f832fad9",
   "metadata": {},
   "source": [
    "### Step 1: Loading and Preprocessing the Novel Text\n",
    "\n",
    "In this step, we load the raw text of *Alice in Wonderland* using the NLTK library's Gutenberg corpus. The text is then manually split into individual sentences using regular expressions. Each sentence is subsequently tokenized into words, converting them to lowercase and removing punctuation. This preprocessing allows us to represent the text as transactions, where each transaction corresponds to a sentence represented by its constituent words. The first few transactions are printed to verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/Importing necessary libraries.\n",
    "import re\n",
    "from nltk.corpus import gutenberg\n",
    "import pandas as pd\n",
    "\n",
    "# Download Gutenberg corpus\n",
    "import nltk\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e213a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw text of the novel\n",
    "alice_text = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# Manually split the text into sentences using regular expressions\n",
    "sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', alice_text)\n",
    "\n",
    "# Tokenize each sentence into words\n",
    "# Write your code here!\n",
    "\n",
    "# Print the first few transactions\n",
    "# Write your code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c77ef08",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "# Tokenize each sentence into words\n",
    "transaction_sentences = [re.findall(r'\\b\\w+\\b', sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Print the first few transactions\n",
    "print(transaction_sentences[:5])\n",
    "\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ee64ed",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **Regular Expressions**: The regex pattern used for splitting sentences accounts for various punctuation and sentence-ending criteria, ensuring accurate sentence segmentation.\n",
    "- **Tokenization**: The `re.findall` function extracts word tokens from each sentence while converting them to lowercase, preparing the data for further analysis.\n",
    "- **Transaction Representation**: The resulting list, `transaction_sentences`, serves as a foundation for further steps in the analysis, such as finding frequent itemsets or generating association rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b677cc1",
   "metadata": {},
   "source": [
    "### Step 2: Prepare Data for the Apriori Algorithm\n",
    "\n",
    "To apply the **Apriori algorithm**, we need to convert the tokenized sentences into a format that can be used for association rule mining. We'll use the **TransactionEncoder** from **mlxtend** to transform the tokenized sentences into a format suitable for the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f348f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/Importing necessary libraries.\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Use TransactionEncoder to prepare data for Apriori\n",
    "# Write your code here!\n",
    "te = TransactionEncoder()\n",
    "\n",
    "\n",
    "# Convert the encoded data into a DataFrame\n",
    "# Write your code here!\n",
    "\n",
    "# Show a sample of the dataframe\n",
    "# Write your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c68b7",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "# Use TransactionEncoder to prepare data for Apriori\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transaction_sentences).transform(transaction_sentences)\n",
    "\n",
    "# Convert the encoded data into a DataFrame\n",
    "df_apriori = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Show a sample of the dataframe\n",
    "print(df_apriori.head())\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908432a",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **TransactionEncoder**: This encodes the tokenized sentences into a binary matrix where each row represents a sentence (transaction), and each column represents a unique word (item).\n",
    "- **df_apriori**: The resulting DataFrame has sentences as rows and unique words as columns. The value in each cell is either `True` (if the word is present in the sentence) or `False` (if it is not)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efb888",
   "metadata": {},
   "source": [
    "### Step 3: Apply the Apriori Algorithm\n",
    "\n",
    "Next, we will apply the **Apriori algorithm** using the **mlxtend** library to discover frequent word patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3508281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/Importing necessary libraries.\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Apply the Apriori algorithm with a minimum support threshold\n",
    "# Write your code here!\n",
    "\n",
    "# Display the frequent itemsets\n",
    "# Write your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c499d66c",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "# Apply the Apriori algorithm with a minimum support threshold\n",
    "frequent_itemsets = apriori(df_apriori, min_support=0.1, use_colnames=True)\n",
    "\n",
    "# Display the frequent itemsets\n",
    "print(frequent_itemsets.head())\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d9f18",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **min_support=0.1**: This sets the minimum support to 10% of the transactions (sentences). You can adjust this value based on how frequent you want the patterns to be.\n",
    "- **frequent_itemsets**: This DataFrame contains the frequent word patterns and their support values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01eb37",
   "metadata": {},
   "source": [
    "### Step 4: Generate Association Rules\n",
    "\n",
    "After finding the frequent itemsets, we generate association rules to identify interesting word combinations or patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d16074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules from the frequent itemsets\n",
    "# Write your code here!\n",
    "\n",
    "# Display the first few rules\n",
    "# Write your code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82420e37",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "# Generate association rules from the frequent itemsets\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "# Display the first few rules\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head())\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d29e95d",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **association_rules()**: This function generates rules from the frequent itemsets. It uses the **lift** metric to identify interesting associations.\n",
    "- **antecedents** and **consequents**: These represent the \"if-then\" rules in the data. For example, if word A is present (antecedent), then word B (consequent) is likely to occur as well.\n",
    "- **support**, **confidence**, and **lift**: These are key metrics used to evaluate the quality of the rules. Higher values of confidence and lift indicate stronger relationships between words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73640beb",
   "metadata": {},
   "source": [
    "### Step 5: Report Interesting Patterns\n",
    "\n",
    "Once you have the rules generated by the Apriori algorithm, you can report interesting word patterns. Based on *Alice in Wonderland*, some common patterns might include famous character names or phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf2162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display interesting word patterns (association rules with high confidence and lift)\n",
    "# Write your code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740bc12f",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "interesting_rules = rules[(rules['confidence'] > 0.6) & (rules['lift'] > 1.2)]\n",
    "print(interesting_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed247fe3",
   "metadata": {},
   "source": [
    "The results you shared show **association rules** that were generated using the **Apriori algorithm** with high confidence and lift values. Let’s break down what each column in the result means and how to interpret it.\n",
    "\n",
    "### Breakdown of Columns:\n",
    "\n",
    "1. **Antecedents**:\n",
    "   - These are the items (words or word combinations) that appear in the left-hand side (LHS) of the association rule.\n",
    "   - For example, in the first row, **(a)** is the antecedent. This means the rule is looking at sentences that contain the word **\"a\"**.\n",
    "\n",
    "2. **Consequents**:\n",
    "   - These are the items (words or word combinations) that appear in the right-hand side (RHS) of the association rule.\n",
    "   - In the first row, the consequent is **(and)**, meaning that if the word **\"a\"** appears in a sentence, then the word **\"and\"** is likely to appear as well.\n",
    "\n",
    "3. **Support**:\n",
    "   - This indicates the proportion of sentences in the dataset that contain both the antecedent and the consequent.\n",
    "   - For example, in the first row, the support is **0.307214**, meaning that around **30.72%** of the sentences contain both the word **\"a\"** and the word **\"and\"**.\n",
    "\n",
    "4. **Confidence**:\n",
    "   - This represents how often the consequent appears in sentences that contain the antecedent. In other words, it’s the likelihood that if the antecedent is present, the consequent will also be present.\n",
    "   - In the first row, the confidence is **0.688022**, meaning that when the word **\"a\"** appears, there's a **68.80%** chance that the word **\"and\"** will also appear in the same sentence.\n",
    "\n",
    "5. **Lift**:\n",
    "   - Lift measures how much more likely the consequent is to appear when the antecedent is present, compared to when the antecedent is not present. It tells you the strength of the association.\n",
    "   - A **lift** value greater than **1** indicates a strong positive association between the antecedent and consequent.\n",
    "   - In the first row, the lift is **1.298521**, meaning that the word **\"and\"** is about **1.30 times** more likely to appear in a sentence when the word **\"a\"** is present compared to its general occurrence in the text.\n",
    "\n",
    "### Interpreting the Results:\n",
    "\n",
    "- **Row 1**: The association rule says that if the word **\"a\"** appears in a sentence, there’s a **68.80%** chance that the word **\"and\"** will also appear in that sentence. This rule has a lift of **1.30**, meaning the presence of **\"a\"** increases the likelihood of seeing **\"and\"** by **30%** compared to random chance.\n",
    "  \n",
    "- **Row 2**: If **\"as\"** is present in a sentence, there's a **61.14%** chance that **\"a\"** will also be present. This rule has a lift of **1.37**, meaning the word **\"a\"** is **37%** more likely to appear in sentences containing **\"as\"** than randomly.\n",
    "\n",
    "- **Row 6936**: Here, a multi-word pattern is involved. If the words **\"the,\" \"a,\" \"to,\" \"she\"** are present in a sentence, then there’s a **69.83%** chance that the words **\"and, it\"** will also be present. The lift of **2.49** means this combination of words is almost **2.5 times** more likely to occur together than by chance.\n",
    "\n",
    "- **Row 6946**: When the words **\"she, a, it\"** appear in a sentence, there’s a **73.64%** chance that the words **\"the, and, to\"** will also appear. The lift of **2.15** indicates a strong association between these two sets of words.\n",
    "\n",
    "### Key Insights:\n",
    "- The rules in the results with high **confidence** and **lift** show strong associations between words or word combinations.\n",
    "- Words like **\"a\"**, **\"the\"**, **\"and\"**, **\"she\"**, and **\"it\"** are commonly associated with each other, forming strong patterns.\n",
    "- **Multi-word patterns** (antecedents and consequents with multiple words) also provide insights into frequent phrases or structures in the text. For example, combinations like **\"the, a, to, she\"** and **\"and, it\"** show frequent co-occurrence of these common words in sentences.\n",
    "\n",
    "### How to Use This Information:\n",
    "- These rules help identify frequently occurring word pairs or patterns in *Alice in Wonderland*. For instance, discovering that **\"a\"** and **\"and\"** frequently occur together could provide insight into sentence structure or commonly used phrases in the novel.\n",
    "- Rules with a high lift value highlight particularly strong associations. For example, the rule **\"she, a, it\" → \"the, and, to\"** is notable for having a strong co-occurrence in sentences.\n",
    "- This kind of analysis can be used to detect stylistic patterns or recurring themes in a literary text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630370c2",
   "metadata": {},
   "source": [
    "### Step 6 : Exporting and Loading Association Rules\n",
    "\n",
    "In this step, we first export the DataFrame containing the interesting association rules to a CSV file for easy access and future use. The rules are saved in a file named **`alice_association_rules.csv`**. After exporting, we read the CSV file back into a new DataFrame to confirm that the data has been saved correctly and can be accessed as needed. Finally, we print the loaded DataFrame to verify the contents and ensure that the data integrity is maintained during the export and import processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd943b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file back into a DataFrame\n",
    "# Install/Importing necessary libraries.\n",
    "import pandas as pd\n",
    "\n",
    "# Export the rules DataFrame to a CSV file\n",
    "# Write your code here!\n",
    "\n",
    "# Load the CSV file\n",
    "# Write your code here!\n",
    "\n",
    "# Print the loaded rules to confirm\n",
    "# Write your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f59eba",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "# Export the rules DataFrame to a CSV file\n",
    "interesting_rules.to_csv('alice_association_rules.csv', index=False)\n",
    "\n",
    "# Load the CSV file\n",
    "loaded_rules = pd.read_csv('alice_association_rules.csv')\n",
    "\n",
    "# Print the loaded rules to confirm\n",
    "print(loaded_rules)\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bab485",
   "metadata": {},
   "source": [
    "## Part 2: Neural Network on MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289837fa",
   "metadata": {},
   "source": [
    "### Dataset:\n",
    "The **MNIST dataset** is a widely-used dataset for digit classification tasks. It contains 60,000 training images and 10,000 test images of handwritten digits (0–9). Each image is 28x28 pixels in grayscale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8c3368",
   "metadata": {},
   "source": [
    "### Step 1: Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a20145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/Importing necessary libraries.\n",
    "!pip install optree\n",
    "!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37259d5c",
   "metadata": {},
   "source": [
    "### Step 2: Load the MNIST Dataset\n",
    "Use the **tensorflow** or **keras** library to load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699f3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess data: Normalize the images and one-hot encode the labels\n",
    "X_train = X_train.reshape(X_train.shape[0], 28*28).astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28*28).astype('float32') / 255\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d5f6fa",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "# Preprocess data: Normalize the images and one-hot encode the labels\n",
    "X_train = X_train.reshape(X_train.shape[0], 28*28).astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28*28).astype('float32') / 255\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d58643f",
   "metadata": {},
   "source": [
    "### Step 3: Define Neural Network (NeuralNetMLP)\n",
    "Here we create a simple neural network model with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763924d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/Importing necessary libraries.\n",
    "!pip install optree\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the NeuralNetMLP model\n",
    "# Write your code here!\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "# Write your code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddb4af6",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "# Define the NeuralNetMLP model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(28*28,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04195d3f",
   "metadata": {},
   "source": [
    "### Step 3: Train and Evaluate the Model\n",
    "Train the model on the MNIST dataset and report its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e468608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Write your code here!\n",
    "\n",
    "# Evaluate the model\n",
    "# Write your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717f7df5",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {score[1]*100:.2f}%\")\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb14363",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "- In Part 1, you explored the Apriori algorithm to find interesting word patterns in *Alice in Wonderland*. \n",
    "- In Part 2, you implemented a two-layer neural network for digit classification using the MNIST dataset and achieved a high accuracy score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
