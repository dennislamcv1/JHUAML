{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7611384c",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"JHU.png\" width=\"200\" alt=\"Johns Hopkins University logo\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff9e47",
   "metadata": {},
   "source": [
    "# Hands-on Lab: Data Features & Model Evaluation\n",
    "\n",
    "Estimated time needed: **60** minutes\n",
    "\n",
    "### Overview:\n",
    "\n",
    "The primary objective of this assignment is to explore and compare various machine learning classifiers and implement a correlation analysis program from scratch.\n",
    "\n",
    "In this lab, we will:\n",
    "\n",
    "- **Classify and Compare:** Discuss and compare classifiers such as Perceptron, SVM, Decision Tree, and Random Forest based on criteria like speed, robustness, and the type of features they utilize.\n",
    "\n",
    "- **Correlation Analysis:** Implement a correlation matrix from scratch for the Admission_Predict.csv dataset, analyzing the relationships between features to determine which are most predictive of the target variable.\n",
    "\n",
    "This lab is designed to deepen your understanding of machine learning concepts through practical application and comparison of different models and techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435a5c2",
   "metadata": {},
   "source": [
    "### Learning Objectives:\n",
    "\n",
    "In this lab, we aim to achieve the following objectives:\n",
    "\n",
    "- **Explore and Compare Classifiers:** Provide a high-level overview of various machine learning classifiers including Perceptron, SVM, Decision Tree, and Random Forest. We will compare these classifiers based on criteria such as speed, robustness, and their suitability for different types of features.\n",
    "\n",
    "- **Implement Correlation Analysis:** Develop a correlation matrix from scratch for the Admission_Predict.csv dataset. This analysis will help identify the relationships between different features and determine which variables are most predictive of the target variable, 'Chance of Admit'.\n",
    "\n",
    "These objectives are designed to enhance your understanding of key machine learning concepts and their practical application in data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caa9a03",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "Let us first explore and compare four essential machine learning classifiers: \n",
    "- Perceptron\n",
    "- Support Vector Machine (SVM)\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "\n",
    "The focus will be on providing a high-level understanding of these classifiers, highlighting their strengths, weaknesses, and typical use cases.\n",
    "\n",
    "As we progress, We will guide you through the following aspects for each classifier:\n",
    "\n",
    "- Speed: We will discuss how quickly each classifier processes data, particularly in large datasets.\n",
    "- Strength: We’ll evaluate the effectiveness of each classifier in terms of its accuracy and ability to handle complex data.\n",
    "- Robustness: We'll consider how well each classifier performs when dealing with noisy or previously unseen data.\n",
    "- Feature Type: Understanding the type of features that each classifier naturally works with, whether they are numerical, categorical, etc.\n",
    "- Statistical Nature: We’ll explore whether the classifier is based on statistical principles.\n",
    "- Optimization Problem: Finally, we’ll examine whether each method solves an optimization problem and, if so, what the associated cost function is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd50aee",
   "metadata": {},
   "source": [
    "**Perceptron:**\n",
    "- Speed: Fast to train, especially on linearly separable data.\n",
    "- Strength: Effective for simple, linearly separable problems.\n",
    "- Robustness: Not robust; struggles with non-linearly separable data and noise.\n",
    "- Feature Type: Works naturally with numerical features, as it relies on calculating weighted sums (essentially a linear combination of inputs).\n",
    "- Statistical: Not typically considered a statistical method.\n",
    "- Optimization Problem: Yes, it solves an optimization problem by minimizing classification errors. The cost function is a simple linear loss function (often using the perceptron criterion, which adjusts weights for misclassified points).\n",
    "- When to Use: It’s a good starting point for very simple problems or as a baseline in scenarios where you expect a linear relationship.\n",
    "\n",
    "**Support Vector Machine(SVM):**\n",
    "- Speed: Slower to train compared to the Perceptron, especially on large datasets or when using non-linear kernels.\n",
    "- Strength: Highly effective for both linear and non-linear problems, especially in high-dimensional spaces.\n",
    "- Robustness: Robust, particularly with well-separated data. It handles outliers better with the soft-margin version.\n",
    "- Feature Type: Naturally uses numerical features, especially those that are scaled appropriately. With kernel tricks, it can handle more complex relationships.\n",
    "- Statistical: Not primarily statistical, but it can be connected to statistical learning theory.\n",
    "- Optimization Problem: Yes, it solves a quadratic optimization problem. The cost function is the hinge loss, which aims to maximize the margin between classes while minimizing classification errors.\n",
    "- When to Use: Ideal when you expect complex boundaries between classes or when you have a high-dimensional dataset.\n",
    "\n",
    "**Decision Tree:**\n",
    "- Speed: Fast to train and make predictions, especially for smaller datasets.\n",
    "- Strength: Intuitive and interpretable; can handle both numerical and categorical data naturally.\n",
    "- Robustness: Prone to overfitting, especially with deep trees, but robustness can be improved with pruning or ensemble methods.\n",
    "- Feature Type: Can handle both numerical and categorical features well. It splits data based on feature values that best separate classes.\n",
    "- Statistical: Not typically statistical but can be interpreted in a probabilistic way.\n",
    "- Optimization Problem: Implicitly solves an optimization problem by maximizing information gain (or minimizing impurity) at each split.\n",
    "- When to Use: A good first choice when interpretability is important or when you have a mix of feature types.\n",
    "\n",
    "**Random Forest:**\n",
    "- Speed: Slower to train than a single decision tree but can be parallelized. Faster to predict than SVMs on large datasets.\n",
    "- Strength: Very strong performance across various datasets, reducing overfitting compared to individual decision trees.\n",
    "- Robustness: Highly robust due to averaging the results of many decision trees, which reduces variance and overfitting.\n",
    "- Feature Type: Naturally handles both numerical and categorical features, similar to decision trees.\n",
    "- Statistical: Can be considered statistical in the sense that it aggregates multiple models (decision trees) to improve prediction stability.\n",
    "- Optimization Problem: Does not solve a traditional optimization problem in the way SVM or Perceptron does. It uses bagging (Bootstrap Aggregating) and random feature selection to build and aggregate multiple decision trees.\n",
    "- When to Use: A great first choice for most real-world datasets due to its versatility, robustness, and strong performance.\n",
    "\n",
    "**Comparison Summary:**\n",
    "- Speed: Perceptron > Decision Tree > Random Forest > SVM\n",
    "- Strength: Random Forest ≈ SVM > Decision Tree > Perceptron\n",
    "- Robustness: Random Forest > SVM > Decision Tree > Perceptron\n",
    "- Feature Type: Perceptron and SVM prefer numerical; Decision Tree and Random Forest handle both types well.\n",
    "- Statistical Nature: Random Forest and Decision Trees can be viewed as more statistical in nature, while Perceptron and SVM are more optimization-focused.\n",
    "- Optimization Problem: Perceptron and SVM explicitly solve optimization problems, while Decision Trees and Random Forests do not in a traditional sense.\n",
    "\n",
    "**Which One to Try First?**\n",
    "\n",
    "Random Forest is generally a strong first choice for most datasets. It is robust, handles both numerical and categorical data, and tends to perform well across a wide range of problems with minimal tuning. If you need something quick and simple for linearly separable data, you might start with a Perceptron, but for most real-world scenarios, Random Forest offers a good balance of performance, interpretability, and ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8653124d",
   "metadata": {},
   "source": [
    "### Understanding Feature Types in Datasets:\n",
    "\n",
    "In this section, we will delve into the various types of features that are commonly encountered in datasets. Understanding these feature types is crucial for selecting appropriate preprocessing techniques and machine learning models.\n",
    "\n",
    "We will cover the following feature types:\n",
    "\n",
    "- Numerical \n",
    "- Nominal \n",
    "- Date\n",
    "- Text\n",
    "- Image\n",
    "- Dependent Variable\n",
    "\n",
    "For each feature type, we will provide examples drawn from existing datasets, such as the Iris dataset, or create examples ourselves to illustrate how these features appear in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af69f32",
   "metadata": {},
   "source": [
    "**1. Numerical:** Numerical features are quantitative and represent measurable quantities. They can be continuous (e.g., any real number) or discrete (e.g., integers).\n",
    "\n",
    "Example Values:\n",
    "\n",
    "From the Iris dataset: Sepal Length (e.g., 5.1, 4.9, 4.7), Petal Width (e.g., 0.2, 0.4, 0.1).\n",
    "\n",
    "A hypothetical dataset might include Age (e.g., 25, 42, 37) or Income (e.g., 45,000, 60,000, 30,000).\n",
    "\n",
    "\n",
    "**2. Nominal:** Nominal features are categorical and represent distinct categories or labels without any inherent order.\n",
    "\n",
    "Example Values:\n",
    "\n",
    "From the Iris dataset: Species (e.g., Setosa, Versicolor, Virginica).\n",
    "\n",
    "Another example might include Color (e.g., Red, Blue, Green) or City (e.g., New York, London, Tokyo).\n",
    "\n",
    "**3. Date:**\n",
    "Date features represent dates or times, typically in a standardized format.\n",
    "\n",
    "Example Values:\n",
    "\n",
    "In a sales dataset: Purchase Date (e.g., 2024-08-29, 2023-05-14).\n",
    "\n",
    "In a weather dataset: Observation Time (e.g., 2024-08-29 14:30:00).\n",
    "\n",
    "**4. Text:**\n",
    "Text features consist of free-form text data, often used for natural language processing (NLP) tasks.\n",
    "\n",
    "Example Values:\n",
    "\n",
    "In a customer review dataset: Review Text (e.g., \"Great product, highly recommend!\", \"The service was terrible.\").\n",
    "\n",
    "In a news dataset: Article Body (e.g., \"The stock market experienced significant gains today...\").\n",
    "\n",
    "**5. Image:**\n",
    "Image features are visual data, usually represented as arrays of pixel values. In machine learning, images are often used in computer vision tasks.\n",
    "\n",
    "Example Values:\n",
    "\n",
    "From the MNIST dataset: Handwritten digit images (e.g., an image representing the digit '5').\n",
    "\n",
    "In a medical imaging dataset: X-ray Image (e.g., an image of a chest X-ray).\n",
    "\n",
    "**6. Dependent Variable:**\n",
    "The dependent variable, also known as the target or response variable, is the outcome that the model aims to predict.\n",
    "\n",
    "Example Values:\n",
    "\n",
    "From the Iris dataset: Species (e.g., Setosa, Versicolor, Virginica), where the goal is to predict the species based on other features.\n",
    "\n",
    "In a housing price dataset: House Price (e.g., 300,000, 450,000, 250,000), where the model predicts the price based on features like size, location, etc.\n",
    "\n",
    "Example Dataset Overview:\n",
    "To illustrate all these feature types, here’s a hypothetical combined dataset:\n",
    "\n",
    "| ID | Sepal Length (cm) | Species    | Purchase Date | Review Text    | Product Image           | Price (USD) |\n",
    "|----|-------------------|------------|---------------|----------------|-------------------------|-------------|\n",
    "| 1  | 5.1               | Setosa     | 2024-08-29    | Great product  | Image of product (file) | 300,000     |\n",
    "| 2  | 4.9               | Versicolor | 2023-05-14    | Not satisfied  | Image of product (file) | 450,000     |\n",
    "| 3  | 4.7               | Virginica  | 2022-03-22    | Would buy again | Image of product (file) | 250,000     |\n",
    "\n",
    "\n",
    "In this example:\n",
    "\n",
    "- Sepal Length is a numerical feature.\n",
    "- Species is a nominal feature.\n",
    "- Purchase Date is a date feature.\n",
    "- Review Text is a text feature.\n",
    "- Product Image is an image feature.\n",
    "- Price is a numerical feature and could be the dependent variable if we were predicting housing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16f4bcd",
   "metadata": {},
   "source": [
    "### Problem statement\n",
    "\n",
    "**Implement a correlation program from scratch to analyze the correlations between the features in the Admission_Predict.csv dataset file. This dataset contains 9 features and 500 data points, and it is sourced from Kaggle. Remember, you are not allowed to use NumPy functions such as mean(), stdev(), cov(), etc. You may use DataFrame.corr() only to verify the correctness of your manually implemented correlation matrix.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150cba10",
   "metadata": {},
   "source": [
    "### Implementation:\n",
    "\n",
    "To implement a correlation program from scratch, you will need to calculate the correlation coefficients between the features of the dataset without relying on functions like mean(), stdev(), or cov() from libraries like NumPy.\n",
    "\n",
    "**Steps to Calculate Pearson Correlation Coefficient:**\n",
    "- Mean Calculation: Compute the mean of each feature.\n",
    "- Standard Deviation Calculation: Compute the standard deviation of each feature.\n",
    "- Covariance Calculation: Compute the covariance between each pair of features.\n",
    "- Correlation Coefficient Calculation: Use the formula for Pearson correlation.\n",
    "\n",
    "**Pearson Correlation Coefficient Formula:**\n",
    "\n",
    "$$ \\rho(X, Y) = \\frac{\\text{cov}(X, Y)}{\\sigma_X \\sigma_Y} $$\n",
    "\n",
    "where \n",
    "\n",
    "**$ \\text{cov}(X, Y) $** is the covariance of **$ X \\text{ and } Y $**\n",
    "\n",
    "**$ \\sigma_X \\text{ and } \\sigma_Y $** are the standard deviations of **$ X \\text{ and } Y $**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e47f2a",
   "metadata": {},
   "source": [
    "Let’s start by loading the dataset and implementing these steps in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7700dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Admission_Predict.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e8ed5c",
   "metadata": {},
   "source": [
    "The dataset contains the following features:\n",
    "\n",
    "- Serial No.\n",
    "- GRE Score\n",
    "- TOEFL Score\n",
    "- University Rating\n",
    "- SOP (Statement of Purpose)\n",
    "- LOR (Letter of Recommendation)\n",
    "- CGPA\n",
    "- Research\n",
    "- Chance of Admit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9709035b",
   "metadata": {},
   "source": [
    "**Step 1: We'll exclude the Serial No. column as it’s just an identifier and doesn't contribute to the analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d372b313",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "df = df.drop(columns=['Serial No.'])\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db983a5f",
   "metadata": {},
   "source": [
    "**Step 2: Calculate and print the mean for each feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27abf0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def calculate_mean(column):\n",
    "    \n",
    "    \n",
    "# Calculate means for each column\n",
    "    \n",
    "    \n",
    "    \n",
    "# Print the means of each feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f2e376",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "def calculate_mean(column):\n",
    "    total_sum = sum(column)\n",
    "    mean_value = total_sum / len(column)\n",
    "    return mean_value\n",
    "\n",
    "# Calculate means for each column\n",
    "means = {column: calculate_mean(df[column]) for column in df.columns}\n",
    "\n",
    "# Print the means of each feature\n",
    "print(\"Means for each feature:\")\n",
    "for column, mean_value in means.items():\n",
    "    print(f\"{column}: {mean_value}\")\n",
    "```\n",
    " \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c18f989",
   "metadata": {},
   "source": [
    "**Step 3: Calculate and print the standard deviation for each feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727729e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def calculate_std_dev(column, mean):\n",
    "    \n",
    "    \n",
    "    \n",
    "# Calculate standard deviations for each column\n",
    "\n",
    "\n",
    "\n",
    "# Print the standard deviations for each feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51236bf5",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "def calculate_std_dev(column, mean):\n",
    "    variance = sum((x - mean) ** 2 for x in column) / len(column)\n",
    "    std_dev = variance ** 0.5\n",
    "    return std_dev\n",
    "\n",
    "# Calculate standard deviations for each column\n",
    "std_devs = {column: calculate_std_dev(df[column], means[column]) for column in df.columns}\n",
    "\n",
    "# Print the standard deviations for each feature\n",
    "print(\"Standard deviations for each feature:\")\n",
    "for column, std_dev_value in std_devs.items():\n",
    "    print(f\"{column}: {std_dev_value}\")\n",
    "\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a21121",
   "metadata": {},
   "source": [
    "**Step 4: Calculate and print the covariance between each pair of features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a897557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def calculate_covariance(col1, col2, mean1, mean2):\n",
    "    \n",
    "    \n",
    "# Calculate covariance matrix\n",
    "\n",
    "\n",
    "\n",
    "# Print the covariance matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb21729",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "def calculate_covariance(col1, col2, mean1, mean2):\n",
    "    covariance = sum((x - mean1) * (y - mean2) for x, y in zip(col1, col2)) / len(col1)\n",
    "    return covariance\n",
    "\n",
    "# Calculate covariance matrix\n",
    "cov_matrix = {}\n",
    "for col1 in df.columns:\n",
    "    cov_matrix[col1] = {}\n",
    "    for col2 in df.columns:\n",
    "        cov_matrix[col1][col2] = calculate_covariance(df[col1], df[col2], means[col1], means[col2])\n",
    "\n",
    "# Print the covariance matrix\n",
    "print(\"Covariance matrix:\")\n",
    "for col1, covs in cov_matrix.items():\n",
    "    row = [f\"{covs[col2]:.4f}\" for col2 in df.columns]  # Format to 4 decimal places\n",
    "    print(f\"{col1}: {', '.join(row)}\")\n",
    "\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d92f1dc",
   "metadata": {},
   "source": [
    "Now, let's proceed to compute the correlation matrix from scratch.\n",
    "\n",
    "**Step 5: Calculate the correlation matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "corr_matrix = {}\n",
    "for col1 in df.columns:\n",
    "    \n",
    "\n",
    "# Convert correlation matrix to DataFrame for readability\n",
    "\n",
    "\n",
    "# Print the correlation matrix DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda4835",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    " \n",
    "```python\n",
    "corr_matrix = {}\n",
    "for col1 in df.columns:\n",
    "    corr_matrix[col1] = {}\n",
    "    for col2 in df.columns:\n",
    "        if std_devs[col1] == 0 or std_devs[col2] == 0:\n",
    "            corr_matrix[col1][col2] = 0  # To handle division by zero\n",
    "        else:\n",
    "            corr_matrix[col1][col2] = cov_matrix[col1][col2] / (std_devs[col1] * std_devs[col2])\n",
    "\n",
    "# Convert correlation matrix to DataFrame for readability\n",
    "corr_df = pd.DataFrame(corr_matrix)\n",
    "\n",
    "# Print the correlation matrix DataFrame\n",
    "print(\"Correlation matrix:\")\n",
    "print(corr_df)\n",
    "\n",
    "```\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43620924",
   "metadata": {},
   "source": [
    "The correlation matrix has been calculated from scratch and presented above. Each value in the matrix represents the Pearson correlation coefficient between two features in the dataset. The values range between -1 and 1, where:\n",
    "\n",
    "- 1 indicates a perfect positive correlation.\n",
    "- −1 indicates a perfect negative correlation.\n",
    "- 0 indicates no correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd0aa4",
   "metadata": {},
   "source": [
    "### Verification:\n",
    "\n",
    "To ensure the correctness of our implementation, we will compare this from-scratch correlation matrix with the one generated by DataFrame.corr()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511785b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the results using DataFrame.corr()\n",
    "verification_corr_df = df.corr()\n",
    "verification_corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f5bf2",
   "metadata": {},
   "source": [
    "The correlation matrix calculated from scratch matches exactly with the one generated by DataFrame.corr(). This confirms that the from-scratch implementation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb06f3d",
   "metadata": {},
   "source": [
    "### Here are some self-check questions based on the above analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff4038",
   "metadata": {},
   "source": [
    "**1. Should we use 'Serial no'? Why or why not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7298169c",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the answer</summary>\n",
    "\n",
    "No, we should not use 'Serial no' in the correlation analysis. 'Serial no' is simply an identifier for each observation in the dataset and does not carry any meaningful information that could contribute to predicting the target variable, 'Chance of Admit'. Including it in the analysis would not add value and might even skew the results, as it doesn't have any correlation with the other features or the outcome.\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b634c487",
   "metadata": {},
   "source": [
    "**2. Observe that the diagonal of this matrix should have all 1's; why is this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2ae01",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the answer</summary>\n",
    "\n",
    "The diagonal of the correlation matrix represents the correlation of each feature with itself. Since a feature is always perfectly correlated with itself, the correlation coefficient is 1. Therefore, all diagonal entries in the correlation matrix are 1 by definition. This is a standard property of correlation matrices.\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4fe08e",
   "metadata": {},
   "source": [
    "**3. Since the last column can be used as the target (dependent) variable, what do you think about the correlations between all the variables?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f3ad6d",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the answer</summary>\n",
    "\n",
    "The correlations between the features and the target variable ('Chance of Admit') reveal how strongly each feature is related to the probability of admission. For example, features like 'CGPA' and 'GRE Score' show strong positive correlations with 'Chance of Admit', indicating that higher scores in these areas are associated with a higher chance of admission. Conversely, features with lower correlations, like 'Research', may still contribute to the prediction but are less strongly associated with the target variable.\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c55a65",
   "metadata": {},
   "source": [
    "**4. Which variable should be the most important to try to predict 'Chance of Admit'?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f36a7f",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the answer</summary>\n",
    "\n",
    "Based on the correlation matrix, 'CGPA' appears to be the most important variable for predicting 'Chance of Admit' as it has the highest correlation coefficient with the target variable (0.882). This indicates that CGPA has a strong positive relationship with the likelihood of admission. Therefore, it should be given the most weight when developing a model to predict 'Chance of Admit'.\n",
    "\n",
    " \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686675bb",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "This lab covered key machine learning concepts, including classifier comparisons, feature types, and performance metrics. We implemented a correlation analysis from scratch, identifying 'CGPA' as the most important predictor for 'Chance of Admit'. The exercises reinforced the importance of understanding data and feature relationships in building effective predictive models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
